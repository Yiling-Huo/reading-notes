---
layout: post
title: (Friston, K., 2005). A theory of cortical responses
date: 2023-04-30 17:00
author: Yiling Huo
tags: ['prediction', 'predictive coding', 'cortical responses', 'theory']
journal: Philosophical transactions of the Royal Society B - Biological sciences
link: https://doi.org/10.1098/rstb.2005.1622
---

This is a review proposes a generative model that are embodied in cortical hierarchies which rests on empirical Bayes. 

*Empirical Bayes means that the prior doesn't need to be a priori'ly specified, it can be estimated from the data. Meaning that the system can 'start' with no prior knowledge. 

The author starts form anatomy, and points out that anatomical findings are consistent with the notion that the brain is hierarchically organized, and there are both forward and backward connections between different levels of hierarchy. Forward (lower to higher level) can only pass one level, but backward connections can bypass intermediate levels and connect more higher and more lower levels. These are mostly supported by connections found in vision areas. There are also neurotransmitter findings suggest that forward connections mediate post-synaptic effects through fast AMPA and GABA (xms delays), and backward, modulatory effects can be mediated by NMDA receptors (50ms delay), which means that 'transmitting prediction from higher to lower level' may be slow and 'sending prediction error from lower to higher' may be fast. 

The author then moves on to propose the generative model, starting from inference and learning. 

"Representation: a neuronal response that represents some 'cause' in the sensorium. Causes are simply the states of processes generating sensory data.  u = g(v, θ), where u is sensory input, v is a list of underlying causes in the environment, g() is a function that generates inputs from the causes, theta represents the parameters of the generative model. Theta unlike causes, is fixed quantities that have to be learned. 

The objective is to make inferences about the causes and learn the parameters. A generative model is specified in terms of a prior distributio over the causes p(v; θ) and the generative distributin or likelihood of the inputs given the causes p(u│v; θ). The conditional density of the causes, given the inputs, are given by the recognition model, which is defined in terms of the recognition distribution (Bayes thorem?): p(v│u; θ) = p(u│v; θ)p(v;θ)/v(u;θ). The endpoint of learning is the acquisition of a useful recognition model that can be appplied to sensory inputs by the brain. Estimating the moments (e.g. expectation) of this density corresponds to inference. Estimating the parmeters of the underlying generativne model corresponds to learning. 

Expectation maximization: EM is a coordinate ascent scheme that comprises an E-step and an M-step. The E-step entails finding the conditional expectation of the causes (inference), while the M-step identifies the maximum likelihood value of the parameters (learning). Both adjust the conditional causes and parameters to maximize the same objective function."

The author then went on to explain mathematically what is predictive coding (minimizing free energy) and how to do it in an empirical Bayes way which I completely do not understand. 

The author then explains how this model can be implemented in the cortical system. The author points out that many findings of cortical neural activities are consistent with the consequences of the previously mentioned maths. The author notice that "in this view, cortical hierarchies are trying to generate sensory data from high-level causes. This means the causal structure of the world is embodied in the backward connections. Forward connections simply provide feedback by conveying prediction error to higher levels. In short, forward connections are the 'feedback' connections."

The author then lists findings in neuroimaging that are consistent with this model, including the MMN. The empirical Bayes scheme would suggest that a component of the N1, corresponding to prediction error, is suppressed more efficiently after learning-related plasticity in backward and lateral connections. The MMN is the attenuation of the N1 (Jaaskelainen 2004). The N1 is an index of prediction error (free energy), and it decrease with each subsequent repetition. This decrease discloses the MMN evoked by an oddball stimulus. In this view the MMN is subtended by a positivity that increases with the number of standard. Some interesting things new to me are that the amplitude of MMN can be affected when plasticity is compromised pharmacologically. Ketamine (a non competitive NMDA receptor antagonist) significantly decreases the MMN amplitude, to both pitch and duration. And when plasticity is affected in disorders such as dyslexia and schizophrenia, MMN amplitude also decreases. 