---
layout: post
title: (Friston, K., 2009).  The free-energy principle - a rough guide to the brain?
date: 2024-01-15 10:00
author: Friston, Karl
tags: ['prediction', 'predictive coding', 'free energy', 'theory']
journal: Trends in Cognitive Sciences
link: https://doi.org/10.1016/j.tics.2009.04.005

star: ['predictive coding', 'theory']
year: 2009
---

## Intro
Free energy principle: any adaptive change in the brain will minimize free energy. 

## Free energy and self organization

- Free energy is an information theory quantity that bounds the evidence for a model of data
- Data are sensory inputs and the model is encoded by the brain.
- Free energy is greater than the negative log-evidence or surprise in sensory data. 
- Unlike surprise itself, free energy can be evaluated because it is a function of sensory data and brain states. Under simplifying assumptions, it is just the amount of prediction error. 

### The Bayesian brain

- Mathematically, the difference between free-energy and surprise is the divergence between a probabilistic representation (recognition density) encoded by the agent and the true conditional distribution of the causes of sensory input. 
- This representation enables the brain to reduce free energy by changing its representation which makes the recognition density an approximate conditional density. 
- The free energy principle subsumes the Bayesian brain hypothesis or the notion that the brain is an inference or helmholtz machine. 

### Beyond Bayes to active inference

- To reduce surprise we have to change sensory input.
- According to the free energy principle, our actions should also minimize free energy (IFM??? That forward model is also responsible for action??)
- Action can reduce free energy ie prediction errors by changing sensory input, whereas perception reduces free energy by changing predictions. 
- In this view, perception is enslaved by action to provide veridical predictions that guides active sampling of the sensorium.
    1. agents resist a natural tendency to disorder by minimsing a free energy bound on surprise
    2. this entails acting on the environment to avoid surprises
    3. which rests on making Bayesian inferences about the world

### Neuronal implementation

- the brain has to encode a recognition density with its physical attributes. These have the role of sufficient statistics (numbers that specify a distribution like means and sd)
- precision (reverse variances) that generate uncertainty about states could be encoded in post-synaptic sensitivity or gain.
- according to the free-energy principle: perceptual inference - optimisation of synaptic activity to encode the states of the environment; perceptual learning and memory - optimisation of synaptic connections that encode contingencies and causal regularities; attention - neuromodulatory optimisation of synaptic gain that encodes the precision of states.
- optimising the sufficient statistics of the parameters is formally identical to associative plasticity and optimising the sufficient statistics of precision is similar to the assimilation of prediction error in reinforcement learning schemes. 
- hierarchical dynamic models of the world formally equivalent to empirical Bayesian models in which higher levels provide empirical priors or constraints on lower levels.

## New perspectives

### The neural code, gain and precision

- the brain represents probability distributions over sensory causes. 
- 