---
layout: post
title: (Ryskin & Nieuwland 2023) Prediction during language comprehension - what is next?
date: 2024-03-07 14:00
author: Ryskin, Rachel and Nieuwland, Mante S
tags: ['prediction in language comprehension', 'review', 'cognitive control']
journal: Trends in Cognitive Sciences
link: https://doi.org/10.1016/j.tics.2023.08.003

star: ['prediction', 'prediction and domain general']
year: 2023
---

- Review studies showing that listeners and readers use all manner of contextual information to generate multifaceted predictions about upcoming input. 
- The nature of these predictions may vary between individuals owing to differences in language experience, among other factors. 
- Three unsolved questions: 
    - Is prediction essential to language processing or an optional strategy?
    - Are predictions generated from within the language system or by domain-general processes?
    - What is the relationship between prediction and memory?
    - Does prediction in comprehension require simulation via the production system?

## Why predict?

Everyday conversations are very fast: despite the typical 2-3 words/s speech rate, interlocutors start responding within a quarter of a second after the other finishes, if not sooner. An increasingly popular hypothesis is that people are generally able to keep up with language input by predicting what comes next - by activating the meaning and potentially other aspects of words ahead of time. But the underlying cognitive architecture and mechanism is still unclear.

Deep learning and comp modeling suggest that predicting the next word (or other linguistic units) is what makes models successful in learning human-like language or replicating behavioural and neuroimaging results. Prediction has also been a highly productive explanatory framework across domains of cognitive science (e.g. predictive coding in vision). 

> Predictive coding
>
> 1. Hierarchical organization: sensory inputs are represented at the lowest levels, whereas more abstract information (e.g. word meanings) is represented at higher levels of the hierarchy.
> 2. Top-down predictions are transmitted from higher levels to lower levels of the hierarchy: each level predicts the responses in the level immediately below via feedback connections.
> 3. Bottom-up prediction errors travel from lower levels to higher levels of the hierarchy: each level transmits the discrepancy between the predicted response and the estimated actual response to the level immediately above it via feed forward connections.
> 4. Prediction errors are used to update the response at each level and generate the next prediction, thus allowing the mind to process/infer the current input and continuously adapt to its environment.
> 5. Predictions and errors are carried by distinct populations of functional units.
> 6. Bayesian inference and precision-weighting: the predictive coding algorithm approximates Bayesian inference. 

However, prediction at the computational level need not be implemented by a mechanism with an explicitly predictive objective. Models with different objective functions (e.g. incremental word recognition, homeostasis) can also account for some aspects of predictive behavior in humans ([citation](https://link.springer.com/article/10.3758/s13423-021-01924-x) and [citation](https://doi.org/10.1016/j.brainres.2021.147578)). 

> Prediction in language by measuring human neural activity
>
> - ERPs/EEG/MEG

## Predictions at the computational level

- Computational analysis: humans predict upcoming linguistic input based on internal models of the environment, and update those models based on some comparison between the prediction and the received input. 
- It is unclear whether processing difficulty is a log-linear function of probability in context (as proposed in surprisal theory). 
- Models predicting neural activity (e.g. fMRI data) works better when using representations that incorporate context via a neural network relative to representations of linguistic input that do not (e.g. distributional semantic model)
- There was a prediction vs. integration debate on reading time and ERP data, but compelling evidence has shown that it's prediction.
- Multivariate approaches to analysing neural data have revealed the predictive processes occurring in the moments before the crucial input is received. ([citation](https://doi.org/10.7554/eLife.39061) and [citation](https://doi.org/10.1523/JNEUROSCI.1733-19.2020))

### The input: the context for prediction

- The context is often operationalised as the preceding words in a sentence and their semantic and syntactic properties. 
- Comprehenders consider a broad scope of contextual information: large narrative, pragmatic cues, etc. 

### The output: the content of prediction

- Robust evidence for semantic prediction.
- Prediction appears to be graded and can include many aspects of form and meaning. 
- Role of hierarchical prediction across multiple representational domains and timescales, where higher-level information constrains predictions at lower levels ([citation](https://doi.org/10.1038/s41562-022-01516-2), [citation](https://10.1073/pnas.2201968119), [citation](https://doi.org/10.1162/jocn_a_01467))
- Acoustic features of words appear to be more sharply encoded by the brain when those words are semantically related to their context ([citation](https://doi.org/10.1523/JNEUROSCI.0584-19.2019)). 
- Sensory sampling is increased when uncertainty is higher ([citation](https://doi.org/10.1016/j.neuron.2019.10.019)). 

- It's unclear though whether top-down predictive signals reach sensory-perceptual (visual or auditory) representations. 
- Debate between Delong and Nieuwland

### Constraints and variability in prediction

- Bayesian brain vs. resource-rational accounts
- Resource-rational accounts propose that prediction should be rational inferences within the biological and informational constraints of the human brain. But it's unclear what the constraints are with respect to language. 

- Studies from children, second-language learners, and older adults.

- The internal model of the world where prediction is from cannot be innate and must be learned. A common view is that prediction is not an end in itself but is a means by which the internal model can continuously adapt to more accurately reflect the world. 
- Prediction updates in young children are larger when children have more vocabulary knowledge. And prediction becomes more accurate as children refine their model of the language.
- But alternative account is prediction is a result of language experience: both children and L2 learners. 