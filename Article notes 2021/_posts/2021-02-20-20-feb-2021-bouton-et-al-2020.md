---
layout: post
title: 20 Feb 2021, Bouton et al. (2020)
date: 2021-02-20 14:18
author: Yiling Huo
---
<!-- wp:paragraph -->
<p>A study using behavioral and MEG to study neural activities underlying audio visual integration, in the cases of fusion and combination. Fusion and combination refer to situations where incongruent audio and visual (face) stimuli elicit either a fused or mixed perception of the sound (auditory /aba/ + visual /aga/ = perceptual/ada/; auditory /aga/ + visual /aba/ = perceptual/agba/). Perceptual fusion and combination are called the "McGurk effect". The authors hypothesized that while fusion is a more straight foward solution facing incongruent audio visual input, combination only happens when perceptual fusion fails, and elicits additional activity in the IFG.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>The first experiment involved a repetition task where participants were showed inconsistant audio and lip movement, and were required to repeat whe they heared as quickly as possible. The authors analysed RT in realtion with conditions. (Stimuli in each experimental condition are designed to elicit either fusion or combination perception, based on previous study results. F2 and lip aperture are used to construct a two dimensional feature space, which can predict perceptual fusion vs. combination. eg, /ada/ has similar F2 to /aba/ and lip aperture to /aga/, thus a fusion; but no sound had F2 of /aga/ plus lip aperture of /aba/, thus a combination of /agba/. Only trials where participants did respond with the expected response were included in analysis, same is true for all experiments in this study). Results suggested that combination is slower than both fusion and control (congruent), and no difference in RT between fusion and congruent. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>The second experiment involved a decision task, where a printed syllable preceded the audio-face stimuli, and the participant's task is to decide whether the audio-face coresponded with the printed letters. This experiment confirmed results from the first experiment, and also helped rule out the effect of articulation planning of combined consonants like /agba/.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Then, the authors carried out an MEG study where participants were showed the audio-face stimuli and were asked to choose what they heard among 5 alternatives (audio, visual, fusion/ada/, combination1/agba/, combination2/abga/). The audio-face stimuli in this experiment varies in stimulus onset asychronies from -120ms audio lead to 40ms audio lag. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>From the MEG experiment, the authors drew several results, supported by a range of simple to carefully modeled statictic tests. They found enhanced neural activity in the STS and IFG in combination. They also found enhanced connectivity form and to the IFG in combination through out the experimental trial. Further, they used a general linear model of 6 rois to study time sensitivity and neural determinants of combination and fusion. The glm suggests that incongruence is first processed in the STS, and the audio visual asynchrony is first processed in the IFG. Decoding analysis showed that univariate classification was possible in the STS for conbination vs. congruent, combination vs fusion, and congruent vs fusion, but was possible in the IFG only for combination vs congrent. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>All in all, the authors pictured a model where combination results both from the identification of the incongruence between the auditory and visual features in the STS, and from the detection of fine audio visual asynchronies in the IFG. Fusion and combination arise from a visual pre-activation of the STS followed by the tracking of the asynchrony in the IFG, which is either interrupted by the input of a compativle audio (fusion), or sequentially continued by the input of an incompativle audio (combination). Combination is triggered by a failure of fusion in the STS.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>This study actually gave me some idea of how to intepret some of the results I had from a perceptual judgement task. I was quite busy the previous days applying for stuff, learning statistics, and enjoying holidays(?), so I was delighted to read a immediately useful article the first day I'm returning to this blog.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>Reference</strong></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Bouton, S., Delgado-Saa, J., Olasagasti, I., &amp; Giraud, A. L. (2020). Audio-visual combination of syllables involves time-sensitive dynamics following from fusion failure. <em>Scientific reports</em>, <em>10</em>(1), 1-18.</p>
<!-- /wp:paragraph -->
