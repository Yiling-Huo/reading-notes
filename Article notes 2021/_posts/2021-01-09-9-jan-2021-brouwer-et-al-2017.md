---
layout: post
title: 9 Jan 2021, Brouwer et al. (2017)
date: 2021-01-09 15:48

---
<!-- wp:paragraph -->
<p>A study that used recurrent neural network to simulate N400 and P600 amplitudes from a previous study, based on a Retrieval-Integration account, where the N400 reflects semantic retrieval and the P600 reflects integration of meaning (to context).</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>The introduction started by introducing the "semantic P600", where syntactically sound but semantically anomalous sentences ('The javelin threw the athletes.') elicited no N400 effect but a P600 effect. The paper introduced a single-stream account of the semantic P600 effects, that is, the N400 only reflects meaning retrieval (thus not affected by whether it was the javelin that threw the athletes or the athletes threw the javelin), while the P600 reflects a more general integration process (thus affected by whether the agent and the patient are plausible). The authors aimed to test this account by a recurrent neural network designed to simulate N400 and P600 amplitudes based on this account, and a successful simulation led the authors to confirm their theory. (Note that in this study, results from classical N400 studies like Kutas Hilliard 1980 are interpreted as having a biphasic N400+P600 effect).</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>The simple recurrent network had two modules, a Retrieval module and an Integration module, and in total consisted of five layers: input, retrieval, retrieval output, integration, and integration output. A hidden layer of integration context affects the retrieval and integration layer, and results from the integration layer also feed the integration context layer. Thus, the model will first take a word form, add contextual meaning to retrieve word meaning, then integrate this result with contextual meaning to have a final output of utterance representation (theta role assigned representations). At each layer, activation level of the artificial neurons is calculated. (There were a figure and a formula for the steps of the model and calculation of activation level).</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>The model was trained by materials similar to the study they were aiming to simulate, consisting of active and passive sentences of 6 or 7 words long (something like 'The [noun] [verbed] the [noun].'). Specifically they trained the model to know that in essence all nouns can be both agents or patients of any verb, but there are also some agent/patient pairs that are especially favored for each verb. They trained the model to 100% correctness, meaning that output vectors are always more similar to its target vector than to its target vector of any other item.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Finally, they linked the N400 amplitude to the degree of change that a word induces in the activity pattern of the retrieval layer, where the pattern is established after processing the previous word; and linked the P600 amplitude to the degree of change that a given word's meaning induces in the activity patter of the integration layer, also established after processing the previous word. In total, they did two simulations (using two sets of training material).</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>As they are using cosine similarity/dissimilarity, they z-scored their previous ERP results to compare with the model simulations.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>The simulations successfully captured N400 patterns: no N400 effect in the role reversed condition 'Javelin threw athletes', and N400 effects in the mismatch conditions 'Cook sung meal', both in active voice and passive voice. The simulations also did a good job capturing P600 patterns, showing P600 effects in all three non-standard conditions. However, although the simulated effect size orders are consistent with actual ERP studies for the N40, with passive mismatch &gt; active mismatch &gt; active role reversal, the simulated P600 effect size orders are not entirely consistent: model predicted passive mismatch &gt; active mismatch &gt; active reversal, actual experiments showed a different pattern with active reversal &gt; active mismatch &gt; passive mismatch. However, the authors then made some corrections with their P600 ERP results, substracting preceding N400 effects from the P600 data, they got the same pattern as the simulations.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>(I suppose they used mismatches and role reversion on the verb position because the Dutch sentences they were using all ended with the verb (something like 'The cook had the meal prepared').)</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>I have to say that I comprehended the paper only thanks to my friend who has knowledge in machine learning. Apparently (ok rooky speaking time) a neural network in the machine learning senario can be seen as some formula with some undetermined parameters (and all networks that are called recurrent neural network share the same basis formula, ← which is a crucual piece of information that I didn't previously know so couldn't comprehend what exactly is happening when people train models), and training a model means to let the model process material using a random parameter setting, if this setting gives better results than the previous setting, then this is the new 'best setting' and is to be compared with the next process, and the procedure is repeated until a satisfactory parameter setting is reached. (My small brain always wonders why people that play with computers and people that play with human brains use very similar terminologies, which often leads to semantic competetion in my little brain when I try to learn new stuff.)</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>In the discussion section, the authors proposed that their model also predicts that, the N400 can be modulated by semantic feature overlap between the current word and the precious word (obviously, if we see the N400 as reflecting differences in semantic representations, and in this case it contains both independet word features and contextual features). They were able to say that in their simulations, the N400 difference is mainly elicited by context, because there weren't much semantic overlapping between their target words and their previous words. However this obviously doesn't mean that semantic overlapping *cannot* affect the N400. In fact, they refered to studies that showed N400 effects with two-word priming paradigm to stress the role of semantic feature overlapping. (But then they carried on to say that the N400 in the two-word priming studies should reflect semantic overlapping rather than contextual priming, which confuses me because I thought in a two-word situation, semantic overlap *is* the contextual priming, isn't it? (Like what else could the first word possibly prime the second word with?) I guess I would put the words this way: if, suppose we have two situations where there are identical supralinguistic context (like action scenes etc), this model would predict that the situation where the actual words have more semantic overlapping with the target word would elicit less N400 than the situation where the previous words share less overlapping features with the target word.)</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>References</strong></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Brouwer, Harm, et al. "A neurocomputational model of the N400 and the P600 in language processing." <em>Cognitive science</em> 41 (2017): 1318-1352.</p>
<!-- /wp:paragraph -->
